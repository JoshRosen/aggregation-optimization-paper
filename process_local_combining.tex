\documentclass[12pt]{article}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb, amsmath}
\usepackage{fullpage}

\usepackage{times}
\usepackage{courier}
\usepackage{minted}
\usepackage{url}

\setcounter{secnumdepth}{0} % supresss section numbers

\title{Process Local Aggregation in MapReduce}
\author{Josh Rosen}

\begin{document}
\maketitle

In current versions of Spark, \texttt{combineByKey} shuffles
$O(\text{numMapPartitions} * \text{numReducers})$ blocks over the network.  In
most cases, it would be more efficient to perform a process-local reduction of
all of the blocks processed by a particular worker, since this may
drastically reduce the amount of data we need to shuffle.

Currently, \texttt{combineByKey} performance can degrade when a large number of
map partitions are used: we may transfer $O(\text{numMapPartitions}
* \text{numReducePartitions})$ blocks when ideally we could transfer only
$O(\text{numMappers} * \text{numReducePartitions})$.

Large numbers of map tasks also cause problems for disk-based shuffles: disk
performance can rapidly degrade if the reduce phase causes many seeks to fetch
many small files.

These problems are barriers to efficiently supporting tiny tasks as
a skew-mitigation and load-balancing technique \cite{tinytasks}.

Fault-tolerance is one of the the main difficulties in fixing these problems:
correct solutions need to support speculative execution, failure of map and
reduce tasks, and recomputation of missing shuffle blocks.

\section{Existing Solutions and Proposals}

There have been multiple proposals to address these problems in Hadoop and Spark.

Sailfish \cite{sailfish} introduces an
abstraction that allows map outputs from multiple writers to be batched
together on disk based on which reducer will process them.  Their
$\mathcal{I}$-files abstraction also allows the number of reducers to be
determined at runtime (between the map and reduce phases) by maintaining
indices to support efficient retrieval of key ranges.

For Spark, \texttt{SPARK-751} \cite{SPARK-751} proposes a scheme where shuffle block
writers are shared between multiple map tasks, and on-disk files store the
concatenated output of multiple map tasks.  Within each file, map outputs are
tagged with map identifiers and sequence numbers that allow reducers to skip
duplicated data and ignore output from tasks that did not successfully
register their completion with the master node .  The on-disk files implement
an atomic append operation that flushes and truncates the files to clean up
partial writes from failed mappers.

For Hadoop, \texttt{MAPREDUCE-4502} \cite{MR-4502} is a proposal to add
node-level aggregation, which is implemented by having one map task per node
act as a machine-local aggregator.  Completed non-aggregator map tasks
communicate with the application master to discover aggregator tasks.

Sailfish has been implemented, but these other approaches have not been
integrated into their respective codebases.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{My Proposal}

\subsection{Keeping Map Outputs in Memory}

Hadoop saves its map outputs to HDFS for fault-tolerance.

Spark currently writes its serialized shuffle outputs to disk to reduce the
cost of recomputing shuffled RDDs and to reduce memory usage.  In principle,
this is inexpensive because the shuffle outputs may remain in OS buffer cache
long enough to be read by reducers, but this data will eventually have to hit
disk; this could be a bottleneck in shuffle-intensive workloads.

Instead, we could move this buffering into Spark, allowing us to exert more
control over when and how this data is flushed to disk.  For example, shuffle
blocks could be cached in the BlockStore by default, being spilled to disk
only when memory is needed.

When selecting a victim to spill to disk, the BlockStore can use its knowledge
of shuffle access patterns to make good eviction decisions.  Once a shuffle
block has been read, it won't be re-read unless a reducer fails.  When
a BlockStore is asked to store a number of shuffle blocks, it may infer that
a shuffle phase is being performed and that many of the next accesses will be
to shuffle
blocks.  Finally, non-dirty disk-resident blocks can be evicted without
performing disk writes. Combining these ideas, we can use the following
heuristic eviction order:

\begin{itemize}
        \item Shuffle blocks that have already been sent
        \item Any blocks that are already on disk
%        \item Shuffle blocks received but not yet processed
        \item Shuffle blocks that have not been sent
        \item Non-shuffle blocks, using the regular LRU eviction policy
\end{itemize}

To improve this approach, we can use a background cleaner thread that
proactively flushes blocks to disk.  Whenever a shuffle block has been sent,
it can immediately be dropped to disk.  We could extend this approach to
continuously flush cached datasets to disk as long as there's free disk
bandwidth, allowing for quick eviction of cached datasets (assuming the disk
bandwidth would be otherwise unused, this also gives us free checkpointing).

% I believe that this is called "write behind caching".

As a further optimization, note that a RDD's block's share the same access
pattern: readers will typically access all blocks of an RDD at the same time,
so there are sets of blocks that should be considered equivalent with
respect to the ordering of blocks by most recent access time (in other words,
we have a logical partial ordering of block access times).
%
By flushing blocks beloinging to the same RDD, we can benefit from sequential
scans when reading them back.

% Disk buffer caches in operating systems might make this optimization for
% small files: they can consider evicting all of the blocks corresponding to
% a file, since the cost to read those blocks back sequentially might be
% cheaper.

% Maybe we could "hedge our bets" by dropping half of the blocks from two
% different RDDs, or some other ratio.  We might uncertainty about which
% queries will be re-run in the future, and the cost-savings from
% materialization or caching aren't "all-or-nothing" because our computation
% is independently parallelizable at some level.  So, instead, we maintain
% a probability distribution over acess patterns and optimize for the least
% expected cost given that distribution.

% Another aside: the "fork-join" patterns can exhibit really bad cache
% behavior with the FIFO replacement policy if we're not explicitly
% uncaching the pre-fork dataset after each iteration.
% This could be solved with a form of reference-counting-based GC
% that determines if there are no non-lineage references to an RDD
% and marks its blocks as early eviction candidates (a type of automatic lazy
% uncache).
%
% This is fairly non-trivial to implement in Java, but we might be able
% to hack it together using PhantomReference.  We could return RDDs to
% the client DSL that are actually PhantomReferences to RDDs or something,
% so when the DSL object has no more references, it's GC'd and we get the
% PhantomReference callback, letting us know that the underlying RDD
% has no non-lineage references.  This would be really nasty, though.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Machine-Local Pre-Aggregation}

To reduce the volume of data that needs to be shuffled, we can pre-aggregate
all of the outputs produced on each node.  A good implementation of this
should allow for flexible, just-in-time scheduling of map tasks, and not
require us to make any assumptions about how many (or which) nodes participate
in the map and reduce phases.

In this section, we'll assume that all shuffle data fits in memory (i.e. we
never hit disk); in later sections, we'll extend this to support on-disk
coalescing of shuffle outputs.  For brevity's sake, we'll present the
algorithm for multiple mappers and a single reducer; reducers operate on
disjoint partitions of data, so this approach generalizes to multiple reducers
by instantiating one instance of the algorithm per reducer.

At a high-level, our strategy will aggregate new map outputs into existing map
outputs from the same phase that are stored on the same machine.  Reducers will
still issue requests for each individual map output, but in response they may
receive a regular map output, the combination of several map outputs, or a
small pointer with the address of the combined output that aggregates the
requested output's data.

The following psuedocode sketches this strategy at a high level:

\begin{minted}{scala}
// On the single Master node:
def blockManagerMaster(message) {
   message match {
      CommitBlock(block_id) => {
         // For now, we'll implement a "first writer wins" policy
         // for shuffle blocks, where we only store the first copy.
         return isFirst(block_id)
      }
      GetBlockLocations(block_id) => {
         // ...
      }
   }
}

// On each worker's local BlockManager:
def blockManager(message) {
    message match {
        FetchBlock(block_id) => {
            synchronized (blockInfo(block_id)) {
                block.markRead()
                return block.data
            }
        }
    }
}

def mapTask(partitionId, data, isRecomputation) {
    // Produce the map outputs:
    mapOutput = f(data)
    blockName = "reduce_" + partitionId
    blockInfo = createLocalBlockInfo(blockName)
    synchronized (blockInfo) {  // Lock out readers
       // Commit the block, then decide how to aggregate it:
       ack = blockManagerMaster.send(CommitBlock(blockName))
       combinerBlock = LocalCombinerManager.getCombiner(stageId)
       if (!ack.isFirst) {
         // Another task already computed this block; discard it.
         return
       }
       if (combinerBlock != null && !combinerBlock.wasRead) {
            // Merge into the existing block.
            // We assume that this operation will succeed:
            combinerBlock.accumulate(mapOutput)
            // Store a pointer to the aggregate:
            blockManager.put(AggregatePointer(blockName,
                                              combinerBlock.name))
       } else {
            // Store regularly, then mark as the new combiner block
            blockManager.put(blockName, mapOutput)
            combinerBlock =
               LocalCombinerManager.registerCombiner(stageId,
                                                     blockNamblockName)
       }
    }
}

def reduceTask(partitionId) {
    blocksInAgg = {}
    missingBlocks = {}
    finalAggregate = new Aggregate()
    asynchronously fetch missingBlocks{ block =>
        block match {
            case AggregatePointer(id, target) =>
                blocksInAgg[target] += orig
                // Sanity check: if a pointer is received after the block
                // it references, the referenced block should contain data
                // from the partition that sent the pointer.
            case Aggregate(id, sourceBlocks, data) => {
                finalAggregate.add(data)
                // If the aggregate doesn't have the expected blocks,
                // re-fetch all of those blocks because we had a failure:
                if (sourceBlocks != blocksInAgg[block.id]) {
                   missingBlocks.add(blocksInAgg[block.id] - sourceBlocks)
                }
            }
            case RegularBlock(id, data) => {
                finalAggregate.add(data)
            }
        }
    }
}
\end{minted}

This algorithm handles several failure modes:

\begin{itemize}
   \item \textbf{Mapper failures during reduce:} If a mapper fails while
   reducers have fetched a subset of its outputs, we need to ensure that those
   reducers eventually re-request any missing blocks.  If a mapper fails after
   a reducer has received its pointers to aggregated blocks that haven't been
   fetched, the aggregated block will be marked as missing and recomputed.
   When it receives the recomputed block, the reducer will realize that its
   contents don't match the expected contents that were calculated from the
   pointers, causing the other missing blocks to be recomputed.

   Unfortunately, this adds some latency because the reducer doesn't
   re-request all of missing blocks as soon as the failure occurs.  We could
   address this problem by having the reducer immediately re-request a block's
   pointers if the block's fetch fails due to a crashed mapper.  This would be
   easy to express in an event-driven model block requests return
   immediately with either ``fetch started'' or ``recomputation started''
   messages, and the fetched data is delivered through a callback or Future.

   \item \textbf{Recomputed map outputs:} Since we always
   persist shuffle outputs on disk, shuffle outputs are only recomputed when
   mappers fail.  Our aggregation strategy only aggregates map outputs into
   blocks that have not yet been fetched by reducers (this is why
   BlockManagers track whether blocks have been read).  If multiple map
   outputs are recomputed using the same machine, the recomputed tasks will
   share a new combiner block and be able to benefit from pre-aggregation.

   \item \textbf{Speculative execution:} With speculative execution, multiple
   tasks could be racing to compute the same map output.  There aren't
   significant benefits to caching redundant copies of shuffle blocks, so we
   only aggregate or store the first copy of a map task's output.  To avoid
   races, the Master acts as the coordinator for ``committing'' map outputs.

  % Note: it's possible to extend the protocol to not throw away redundant
  % copies, since this gives us some limited fault-tolerance capability, but
  % the complexity doesn't seem justifiable:

  % This might be addressed by adding a 'commit' phase, where output's
  % availability is registered with the BlockManagerMaster before deciding
  % whether to combine it or not.  If a speculative task completes first, then
  % the original copy of that task won't aggregate its output with any other
  % blocks.

  % This doesn't address the opposite situation, where the speculative task
  % finishes second but the original task's output has already been saved.  We
  % could handle this case by still storing the speculative task, but under
  % a different name.  We would only need to access this block if the mapper
  % that ran the original mapper failed.  To handle this, we can add
  % special-case logic to the BlockManagerMaster to search for this block when
  % the last copy of a shuffle block is removed from its mapping; if the block
  % is found, we can rename it.

\end{itemize}

The above psuedocode assumes that a mapper's combined data can always be
succesfully merged into the per-machine aggregate.  If we're extreme memory
pressure, we could spill some map tasks' output to disk and finish the
pre-aggregation before we deliver those partitions to reducers.

In this approach, mappers don't need to be notified once they have processed
their final map task in a map phase.  State is shared between map tasks, but
that state is stored in the BlockManager, allowing tasks from other jobs to
run concurrently (we don't have to set up and tear down an explicit
``context'' in order to persist and share data across tasks).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Pipelining}

This aggregation scheme can be extended to support a limited form of pipelined
reduce, where reducers can begin fetching outputs from completed mappers while
other mappers are still executing.  We simply add additional locking to ensure
that partial pre-aggregates become immutable once they have been read.  This
avoids problems where a completed map task aggregates its results into a block
that has already been fetched and that may not be re-requested unless
a reducer fails.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Spilling Shuffle Data to Disk}

We can employ similar strategies for spilling shuffle data to disk.  By
keeping shuffle outputs in memory, we will avoid hitting disk for shuffles
with many partitions and little data per partition (one of the
worst-performing cases in terms of bytes of data read per disk seek).  For
many combine-by-key operations, machine-local combining should significantly
reduce the shuffled data volume.

For jobs that do not benefit from combiners, we still may have to spill
shuffle output to disk.  We can employ a similar ``indirection''
scheme, storing pointers such that the combined files on disk are requested
and processed similarly to how we process aggregates.

This still supports aggregation: we can just spill partial pre-aggregates to
disk.

\textbf{TODO:} I haven't thought through the details of the file consolodation
approach, but we could probably borrow Sailfish's approach with a few minor
modifications to account for the fact that we're writing to local disks
instead of a DFS.

%\textbf{TODO:} I haven't thought through disk-based strategies that much,
%since I expect that with this type of pre-aggregation, the jobs that would
%need to spill the partial aggregates to disk would either cause the reducers
%to run out of memory when reading those aggregates, or would run out of memory
%while constructing a hashtable in the mapper.  It may be worth revisiting this
    %if we add more general ``graceful-degradation'' under memory pressure by
        %using disk-spilling algorithms and data structures.

\subsection{Correctness}

As an informal proof of correctness, note that this process-local aggregation
scheme maintains a few invariants:

First, we ensure that there is only one accessible copy of each shuffle block.
For speculative tasks, the master can use the ordering of task completion
messages to commit only the first copy of concurrently-running copies of
a task; in the rare case that two speculative tasks finish in quick succession
(before the second, redundant is killed by the master), the second task will
discard its output and the master won't add it to the list of stored blocks.

We assume that mappers may only exhibit total failures in which they lose all
shuffle blocks.  We still need to handle edge-cases where a node becomes
temporarily inaccessible, is marked as dead by the master, and eventually
re-connects and becomes live again.  To handle this case, we can require the
node to re-register its blocks with the BlockManagerMaster; during this
re-registration, we could drop that node's shuffle blocks.  It would be safe
to re-register the recovered node's shuffle blocks only if none of those
blocks were recomputed elsewhere during the failure; however, this will be
prone to races and difficult to implement.

\subsubsection{Reducers' handling of failures}

The fetching of shuffle outputs is controlled by reducers.  The fetching
protocol maintains the invariant that shuffle outputs delivered to reducers
may always be immediately aggregated without leading to over-counting.  This
is clearly true when there are no failures, because the task completion
``commit'' protocol ensures that each shuffle block is only computed once and
any process-local aggregation is performed only after a successful commit.

This protocol is also correct during mapper failures: over-counting could only
occur if a map task was computed twice, which could only happen if a reducer
requested it twice.  Reducers will only re-request shuffle blocks that they
have not received.  When an aggregated shuffle block is received by a reducer,
it contains the ids of the shuffle blocks that were aggregated into it, so the
reducer won't re-fetch those blocks.  If a reducer receives an aggregated
block whose contents don't match the expected contents (calculated from the
``pointer'' blocks that it received), then a failure occurred and the
aggregate block was recomputed and contains a subset (possibly only one block)
of the blocks that it should have contained.  It's safe for the reducer
to add this block to its aggregate and re-request only the missing blocks.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{plain}
\bibliography{aggregation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
