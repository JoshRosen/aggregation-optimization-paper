\documentclass[12pt]{article}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb, amsmath}
\usepackage{fullpage}

\usepackage{fourier}
\usepackage{courier}

\setcounter{secnumdepth}{0} % supresss section numbers

\title{Aggregation Optimizations}
\author{Josh Rosen}

\begin{document}
\maketitle

\section{Introduction}

Aggregate-by-key is a common and often expensive operation in ``big data'' frameworks like MapReduce and Spark.

There are many techniques to optimize these aggregations, including multi-level aggregation techniques, like aggregation trees, and pre-aggregation techniques, like Hadoop and Spark's map-side combiners.
The aggregation operator's performance is sensitive to skew in its input data properties, such as uneven frequencies of aggregation keys or an uneven partitioning of the keyspace across machines.
Its performance can also be order-sensitive; for example, pre-aggregation schemes benefit from ``clusteredness'' in their input data, when values with the same key appear close together in the stream.

All of these optimizations involve trade-offs between latency, memory, network usage, and processor time, so the optimal aggregation technique is a function of the data properties and these resources' relative costs.
It's also interesting to consider how adaptive query processing techniques might be applied to aggregation in a streaming setting.

The rest of this report summarizes related work in aggregation optimization, proposes some new optimizations, and explores issues related to aggregation and lineage-based fault-tolerance.

\section{Multi-level Aggregation}

Multi-level aggregation techniques, such aggregation trees, can address
several problems:

\begin{itemize}
    \item In wireless sensor networks, the network may not be fully-connected
    and some pairs of nodes must communicate through other intermediate nodes.
    To save bandwidth (which can lead to large power savings), aggregation can
    be performed at intermediate nodes of the spanning tree \cite{tag}.

    \item Even if all pairs of nodes can directly communicate, the underlying
    network topology may not be flat: for example, intra-rack bandwidth can be
    much greater than inter-rack bandwidth.  In these cases, aggregation trees
    can reduce the amount of data communicated through the cross-rack
    bottleneck.

    \item A node's bandwidth is usually shared by all parties communicating
    with it, so aggregation schemes that send all data to a single node may
    become bottlenecked on that machine's individual network link; aggregation
    trees can alleviate this problem by reducing the total amount of data sent
    to any individual machine.
    % TODO: a comment on MPI-style AllReduce might be appropriate somewhere.

    \item For popular keys, aggregation trees can distribute the processing
    cost of applying the aggregate function.

\end{itemize}

Hierarchical aggregation may harm performance if it achieves a low reduction
in network bandwidth. For example, a binary aggregation tree will cause
a logarithmic increase in execution time if the intermediate aggregations
don't reduce the data volume.

\subsection{Aggregation on Multiprocessor Machines}

Multiprocessor machines may produce multiple sets of aggregation outputs and
it is often beneficial to locally combine them.  It may be possible to have
all processors incrementally update the same aggregation data structures, but
this may harm performance through cache invalidation and synchronization
overheads.  Instead, it may be more efficient to give processors their own
aggregation buffers and merge these in a secondary machine-local aggregation
phase.

Sparsity may change these trade-offs: if the key space is large and the
distribution of keys is fairly even, then cache invalidations may already be
common and updates to the shared structures may be less likely to conflict.

\subsection{Network Non-Linearity}

As a simple cost model network communication, we can assume a linear relationship between data size and communication time: halving the amount of data sent between pairs of nodes will halve communication times.
This model breaks down when pairs of nodes are communicating small volumes of data: at the extremes, hardware and OS network stack overheads can result in all packets below some size threshold requiring the same communication time.
This has implications for the scalability of MPI-style all-reduce algorithms that communicate \texttt{1/numMachines}-sized chunks of dense vectors.

\textbf{TODO:} I heard about these effects from John Canny's talk at the Sys/ML lunch; I should find some citations for this.

\section{Pre-aggregation}

In engines like MapReduce, aggregation queries can be executed by
hash-repartitioning the dataset by its grouping attribute(s), then using
a hash-table to compute the aggregates for each group given all of its values.
% TODO: do multiple grouping attributes introduce anything interesting into this scheme?

This can be optimized using \emph{pre-aggregation}, where the aggregation is
performed in two phases: first, the subset of a key's values on a particular
machine are aggregated into a single value; next, these aggregates are
hash-repartitioned and a final aggregation is performed on each group.
This corresponds to Hadoop's ``Combiners'' feature.
For this optimization to apply, the aggregation function has to be commutative
and associative.
% TODO: is this strictly true, or can we use a more-precise or weaker set of properties?

Pre-aggregation can improve performance by reducing the volume of data set
over the network and by distributing the CPU cost of applying the aggregation
function to popular keys.
These benefits come at the cost of additional hash lookups and memory usage
during the pre-shuffle phase.

\subsection{Pre-aggregation and Skew}

Skew can affect pre-aggregation's cost and benefit.  Key-value pairs can exhibit both input and output skew \cite{adaptive-aggregation}.
With \emph{input skew}, all nodes have the same number of groups but different numbers of records.  Input skew can cause a particular node to become a straggler because it has more input data to process.  In contrast, with \emph{output skew}, nodes have the same number of records but different numbers of groups or distinct keys, which means that some nodes may not have enough buffer space to pre-aggregate all groups.

\subsection{Partial Pre-aggregation}

Total pre-aggregation is an ``all-or-nothing'' optimization that performs
total aggregation on each machine's local data, then performs all
communication in a separate shuffle phase.
\emph{Partial pre-aggregation} \cite{partial-preaggregation} relaxes these
constraints: a single machine may produce multiple partial aggregates for each
key, and the shuffling of data between machines can be pipelined.
This works because the two-phase aggregation strategy still needs to perform
a final aggregation, so the partial pre0aggregates will be combined during
the final post-shuffle aggregation.

Partial pre-aggregation overcomes limitations on buffer space for holding the pre-shuffle grouping hash table.
While sequentially scanning through records, values can be aggregated into matching entries in the hashtable or inserted as new entries.
When the hashtable becomes full, partial pre-aggregation algorithms employ an \emph{eviction strategy} to free up space.
The simplest eviction strategies evict a randomly-chosen key or empty the buffer by evicting all keys.
Values can be evicted by spooling them to disk or by sending them over the network to the node that performs the final aggregation for those keys.

This is exactly analagous to cache eviction policies, where we want to maximize cache hits (preaggregations), and algorithms like LRU or LFU can be used.
The optimal eviction strategy is to evict the key that will be hit furthest in
the future \cite{Belady1966}; it's not possible to actually implement this
strategy, but it provides an upper bound on the effectiveness of our eviction
strategy.

\subsection{When to Apply Pre-aggregation}

To decide whether to apply total pre-aggregation or to shuffle the entire
dataset, we can calculate the total number of groups being aggregated, since
pre-aggregation is beneficial when there are relatively few keys, but may harm
performance on data sets with huge numbers of unique or infrequent keys
\cite{adaptive-aggregation}.
Sampling techniques can be used to estimate the number of groups.
Some systems pick an initial aggregation strategy and adaptively change it if
the original cost estimates appear to be wrong (e.g. by having nodes
autonomously switch from total pre-aggregation to direct shuffling of tuples
if they do not have enough buffer space for the pre-aggregation hashtable)
\cite{adaptive-aggregation}.

Partial pre-aggregation is sensitive to the input data ordering, complicating
this trade-off.  If the input is perfectly clustered by the grouping
attributes, then pre-aggregation requires one partial results' worth of buffer
space to achieve the maximum reduction in output size.  Knowing the exact
distribution of values across groups doesn't help here, since an adversarial
input ordering can lead to high eviction rates.

% TODO: What about multi-phase partial sorting for aggregation?
\section{Additional Optimizations for Pre-aggregation}

\subsection{Allocation of Buffer Space}
Buffer space is a limited resource and in a pipelined execution we have to share it among the pre- and post-shuffle hash tables; we can also use buffer space to implement a limited form of ``lookahead'' by dynamically re-ordering arriving tuples before probing the hashtable.  For this analysis, assume that all machines participate in both the pre- and post-shuffle phases.

% Aside: Belady's anomaly shows that increasing buffer space doesn't always improve hit rates / reduce faults under a FIFO replacement policy.  This anomaly doesn't apply to LRU or the optimal replacement policy.  It's worth considering whether the non-optimal heuristics we'll use can suffer from this anomaly.

\subsection{Buffering for Lookahead}

We might consider using a small amount of buffer space to implement ``lookahead'' against the preaggregation hash table.
Disk drives use a similar idea to serve requests out-of-order to reduce seeks or rotational delays.
Similarly, sorting can reduce cache misses.

With both disks and processor caches, the cost of a ``miss'' is extremely high, yet caches are very expensive so a sort buffer may make sense.
% TODO ... but that's not necessarily the case here because ...
% game out the possibilities as a matrix of outcomes

The trade-off here is one record of additional lookahead versus one hashtable slot, plus the CPU overheads of sorting or otherwise managing the lookahead buffer.

There may be related work in compression literature, since sorting (clustering) reduces the entropy in a dataset and improves its compressibility.

In web caching, people have investigated request reordering approaches where incoming requests can be served out of order \cite{web-caching-new-results}.  In that context, we're concerned with not delaying requests for too long.  This notion of ``too long'' seems to be measured in terms of requests, such that the oldest re-ordered request arrivied at most $r$ requests ago.  The general version of this re-ordering problem is NP-hard (why?).  There are different models for how much space documents take in the cache and the cost of cache misses.  The simplest model, corresponding to our hash-table sizing problem, is the Uniform Model, where all documents have the same size and carry the same cache miss penalty.

The reordering buffer management problem \cite{online-scheduling-for-sorting-buffers} considers requests arriving at a service provider that can benefit from contiguous runs of requests for the same item.  The service provider can buffer a finite number of requests and must evict a buffered request for each incoming request.  The goal is to minimize the number of ``context switches'' that the service provider performs by switching the type of item that it's processing.

A generalization of this proble, the \emph{multi service sorting buffer} problem, has a set of idential service providers.  If requests can't be buffered, this reduces to the classic \emph{paging problem}.  The \emph{queue sorting buffer} problem is another variant where incoming items are appended to queues with fixed space and the service provider must process the head of one of the queues (akin to mergesort).

More generally, our problem appears to be a type of \emph{sorting buffer} or \emph{reordering buffer} problem.

Aha, so the problem that I'm trying to solve is NP-hard since it's the \emph{generalized sorting buffer problem} \cite{sorting-buffer-np-hardness}.
Longest Forward Distance is the optimal offline algorithm for the paging problem \cite{lfd}, but it's not a constant approximation for the sorting buffer problem \cite{sorting-buffer-np-hardness}:

The paper \cite{competitive-reordering-algorithm} considers...

% TODO: have these strategies been explored in disk-spilling hashtables?


\subsection{Selective Bypass}
It may be worth investigating work on \emph{caching with bypassing}.  In many caching scenarios, like memory paging in operating systems, there's no option but to evict from the cache.  Our problem of deciding whether to evict a key or directly forward an input tuple is exactly analgous to the problem of caching with bypassing.

Again, we're faced with a trade-off: if we maintain a list of records with poor hit ratios, the space used to maintain that list could also be used to have a bigger cache.  In processors, you can classify individual instructions in a loop as ``tends to cause cache misses`` (or hits), so you only need a small dense array to track hit rate information \cite{automatic-cache-bypass}.

% TODO: a large portion of the literature here seems to be in patents.

% It looks like a relevant search term is "adaptive cache manamgement strategies"

% TODO: parallelism: can you pipeline this process, having one thread sort pages of records at a time while another thread consumes sorted pages and probes the hashtable?

% TODO: what about associativity?

% \section{Evicting to Local vs. Remote Disk}

% \section{Breaking Ties When Choosing Victims}

% Maybe we could break ties when choosing victims by preferring victims that have been recently evicted (a sort of ``poor get poorer'' approach).

\pagebreak

\section{Recovery}

\emph{(This idea is more general, but I'm just using Spark as an example)}

These aggregation optimizations may require extensions to Spark's lineage-based failure-recovery mechanisms.

Let's say that we're trying to recover a single lost partition of an
aggregate-by-key result.  This case is somewhat uninteresting because the lost
partition possibly depends on all partitions of the un-aggregated data set.
As a trivial optimization, during recomputation we can only aggregate keys
that hash into the lost partition.

\emph{(I had an old idea here related to avoiding double-counting of
aggregated accumulator updates when tasks are recomputed, but this is
trivially solvable by associating a ``generation'' number with each round of
recomputation and only partially combining updates belonging to the same
generation.  This solves a few design problems related to accumulator
optimizations in Spark, but it's kind of a tangent.)}

\subsection{Query Planning During Recovery}

The query processor has a few options for handling runtime failures: it can
restart the entire query, try to recompute only the lost partitions, or ignore
the failure and return a lower-quality result.  This is a non-trivial problem,
because statistics generated during query execution might suggest that an
alternative plan would be faster.

\subsection{Ordering and Materialization}

Spark benefits from caching of base and derived datasets, but it's up to the
user to decide what to cache.
A simple approach to auto-caching would examine the cost of recomputation and
frequency-of-use for each intermediate dataset and pick a subset to
materialize.

Building on ideas from materialized view maintenance in traditional RDBMSs, we
can rewriting queries to be processed using different derived datasets.  For
example, say we have the queries \texttt{base.map(f)} and
\texttt{base.groupByKey(g)}.  If the second query's result is already cached,
we can use that result to process the first query.  If we're only executing
the map query, it doesn't make sense to perform the extra shuffle step.
Similarly, if the shuffled data was lost, it could be recomputed while
processing the map query, but this is only worth doing if we expect the
group-by result to be accessed by another query that requires it.

We have to be careful when changing query plans to recompute lost partitions.
The group-by-key reorders (and compresses) the base data.  The \emph{set} of
results produced by both map query plans is the same, but the ordering of the
results in partitions is different.  Recovery needs to take this into account
to ensure that all of a query's cached partitions are computed from the same
base or derived data sources.  Going further, some jobs might be sensitive to
the ordering of values in partitions, so recomputation must \emph{always}
produce the exact same order of results.  It would be useful if the query
planner knew about these cases, since they restrict its ability to optimize
queries.

There are several query rewrites that Spark doesn't currently employ.  For example, given an RDD \texttt{pairs} of key-value pairs, the query
\texttt{pairs.mapValues(f).groupByKey(g)} can be rewritten to
\texttt{pairs.groupByKey(g).mapValues(\_.map(f))}, avoiding
a potentially-costly shuffle.  This optimization can even be applied if the
grouped pairs aren't cached: if \texttt{f} significantly increases the size of
the values, then it may be cheaper to apply it post-shuffle so we can shuffle
less data.

Additionally, a cached, partitioned dataset can be used as an index for
selection queries.  We can avoid scanning partitions whose values fall outside
of a partition's range.  Shark already applies this ``partition-pruning
optimization''.  To automatically perform this optimization in Spark, we need
a mechanism to inspect \texttt{filter} predicates.  We may also want
conditional query plans that allow the query to fall back on a regular scan if
the relevant parts of the partitioned dataset are lost.

\subsubsection{Cost Model}

Maybe we can adapt DynaMat's \cite{dynamat} view selection policies to decide
which intermediate results to materialize.  Like DynaMat, we're concerned with
both storage and recomputation costs.  Unlike DynaMat, our views are immutable
and don't have a periodic update cost.

DynaMat's cost model incorporates relationships between views: a view's
update cost may be amortized over other the views that it can recompute
(in some cases, this can cause a view to have a negative update cost).
In DynaMat, a \emph{child} view is derived from a \emph{father} view if
it was originally computed from that view.  When calculating change in total
update cost from not recomputing a father, DynaMat accounts for savings in its
children's update costs:
\[
    U_{delta}(f)
    =
    UC(f)
    - \sum\limits_{f_{child} \in \mathcal{V} : father(f_{child}) = f}
    \left(UC^{new}(f_{child}) - UC^{old}(f_{child})\right),
\]
where $UC^{new}$ is the cost of updating the child without using its father,
so the summation is counting the increased child update costs against the cost
savings $UC(f)$ from not updating the father.

A limitation of DynaMat's analysis is that it only considers parent-child
relationships that actually occurred during view materialization, rather than
considering the complete set of possible parents for the child.
% TODO: the above is a little inprecise / confusingly-worded.
This could have bad performance implications for some Spark jobs.  Say that one 
query materializes a filtered subset of the base data it, followed by another
query that materializes a shuffled version of the base data.  Even though the
filter query's result was computed from the base data, we should also consider
that it can be executed on the shuffled data (possibly benefiting from
partition-pruning) when deciding whether to materialize the shuffle output.

Because we don't have a periodic view update window, we can reduce the problem
to deciding which cached results to evict in order to cache new results.
This extends caching heuristics like LRU to account for relationships between
the cached items.

\section{Optimizations for Skewed Data}

\cite{perf-eval-heavy-tail} describes several applications in networking,
load-balancing, and scheduling where awareness of skewed data distribution
enables optimizations.

These optimizations usually work by identifying the most popular keys or data
elements and processing them differently.

\textbf{TODO:} talk about how this trick can be used to adaptively apply many
other types of aggregation optimizations (e.g. shared and synchronized (or
CAS) data structures for infrequent, long-tailed keys, and separate,
cache-locality-preserving structures for the popular keys).

%\subsection{Machine Learning}

%Skew in machine learning: skew can cause problems for classification tasks.
%For example, in fraud detection systems, most transactions are not
%fraudulent, so a naive algorithm that always says ``not fraudulent'' will be
%correct most of the time
%(http://dl.acm.org/citation.cfm?id=1007730.1007738).  In practice,
%techniques can be used to address the imbalance in the underlying data, such
%as performing biased sampling so the fraudulent training examples will be
%weighted more heavily.

\subsection{Join Queries}

There are several join optimizations for skewed data.  For example,
\cite{smj-skew-optmization} modifies sort-merge join to devote more resources
to processing popular keys.
% TODO: I need to read through the SMJ optimization papers in more detail.

Apache Hive \cite{hive-join-strategies} employs a scheme that uses
a shuffle-based join for most keys, but defers joins for the extremely
frequent keys.  These keys are joined in a separate map-join phase.  This
approach prevents a single popular key from bottlenecking at a single reducer.

% Skew Handling Techniques in Sort-Merge Joins: http://www.cs.arizona.edu/people/dgao/pub/skew.pdf

% In the AQP survey framework, this is an adaptive technique that's based on \emph{horizontal partitioning}, picking different traditional query plans for disjoint portions of the query's data.

\pagebreak
\section{Analysis}

\subsection{Payoffs in terms of data properties}

\subsection{Characterization of optimal and default strategies}

\subsection{Permutations of local data}

Consider a data set of $n$ key-value pairs, with $k$ distinct keys.

Assume that keys are drawn from a Zipf distribution with parameter $\alpha$.
Using complete pre-aggregation on data drawn from this distribution, what's the best-case reduction of tuples in the input to the final aggregation phase?
For large values of $\alpha$, the keys are sampled uniformly, so each key appears roughly $n/k$ times and complete pre-aggregation reduces the final reducer's input by a factor of $n/k$.
At the other extreme, a small $\alpha$ produces a handful of popular keys and a long tail of unique or infrequent keys.

Actually, complete pre-aggregation always reduces the size by a factor of $n/k$, since we output exactly one tuple for each of the $k$ keys.

%To be precise, if key $i$ appears with frequency $d_i$, then complete pre-aggregation outputs exactly $k$ tuples, one per key, saving
%\[
%    \sum_{i=1}^k \min \left(d_i - 1, 0\right) = 
%\]

If we draw keys from this Zipf distribution, what's the expected number of unique keys that are assigned to each of $m$ machines?
\begin{align*}
    E[X|\alpha] &= \sum_{k=1}^K E[I(k, d_k) | \alpha]
\\              &= \sum_{k=1}^K \sum_{d=1}^K \left(1 - \left(1 - 1/m\right)^d\right)p(d | \alpha)
\\              &= k \sum_{k=1}^K\left(1 - p\left(\text{all copies of $k$ are assigned to other machines}\right)\right) p(d | \alpha)
\end{align*}

What's the maximum number of unique keys that could be assigned to one machine?  In principle, all keys could appear on one machine, so we'll look at the distribution of the maximum number of keys per machine.


\pagebreak
\section{Tricks}

\subsection{Affinity Routing}

\subsection{Multi-level aggregation with eviction and skip levels}

\subsection{Sketching for frequent values}


\section{Adaptivity}

The optimal aggregation strategy depends on the number of groups and their
distribution across nodes.
For example, it only makes sense to use Combiners in MapReduce when the number of groups is small.
Using global statistics, we could try to estimate the number of groups to decide whether to use combiners.
Alternatively, we can use adaptive aggregation algorithms that allow individual nodes to pick their aggregation strategies based on the properties of their local input data \cite{adaptive-aggregation}.

% TODO: borrowing from other AQP work, maybe we could use only a sample of the input stream to maintain hit rate statistics.

\section{Order Sensitivity}
Preaggregation techniques are sensitive to the arrival order of tuples.  With partial preaggregation, for example, the output size may be minimal if the tuples arrive in sorted order, but can be much larger with other orderings \cite{partial-preaggregation}.

When analyzing the expected output reduction due to partial preaggregation, \cite{partial-preaggregation} assumes that input records arrive in a random order (i.e. their groups are randomly sampled from some (possibly-skewed) distribution).

Fully-sorted input will achieve optimal aggregation performance, but partially-sorted orders are still interesting to analyze.  Often, real-world data exhibits temporal clustering; for example, interaction data from a single user's shopping session is likely to be close together in a clickstream log.  To analyze these cases, \cite{estimating-cardinality} introduces a \emph{clusteredness} measurement and explores its impact on the performance of partial preaggregation algorithms.  The clusteredness metric considers only the clustering of input groups and not their relative sort orderings.

\subsection{Clusteredness}

Find the positions where the group changes.  Add these up and exclude the first appearance of each group.  Then, perfectly clustered data has zero changes.  By itself, this is an insufficient measure of clusteredness because it doesn't consider \emph{which} groups we're changing between (e.g. cycling between a few groups vs. changing uniformly at random).  So, we extend this to incorporate the average number of changes until members of the same group are seen.  In short, the clusteredness ``expresses the average number of changes between the appearance of two identical grouping values.''  A clusteredeness of $d$ means that, on average, $d$ group changes will occur until a group is re-encountered.


\textbf{Distance classes:} distances between appearances may have high variance, so we replace a single average $d$ with a set of average distances over different ranges (this is like creating a histogram using more bins).

\textbf{Group classes:} ???

The paper doesn't explore how to efficiently compute the clusteredness, so I'm doubtful of this technique's usefulness in practice.

% TODO
\textbf{Question}: what's the actual clusteredness of real data sets?  Can we compute this and just report some empirical results, since these would establish more realistic bounds for real problem instances?

\subsection{Bounds}

\cite{estimating-cardinality}
Randomized replacement represents an upper bound on the output size; eviction using lookahead gives a lower bound, even though it's impossible to implement in practice.


\bibliographystyle{plain}
\bibliography{aggregation}

\end{document}
