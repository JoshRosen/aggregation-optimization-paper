\documentclass[12pt]{article}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb, amsmath}
\usepackage{fullpage}

\usepackage{fourier}
\usepackage{courier}

\setcounter{secnumdepth}{0} % supresss section numbers

\title{Aggregation Optimizations}
\author{Josh Rosen}

\begin{document}
\maketitle

\section{Introduction}

Aggregate-by-key is a common and often expensive operation in ``big data'' frameworks like MapReduce and Spark.

There are many techniques to optimize these aggregations, including multi-level aggregation techniques, like aggregation trees, and pre-aggregation techniques, like Hadoop and Spark's map-side combiners.
The aggregation operator's performance is sensitive to skew in its input data properties, such as uneven frequencies of aggregation keys or an uneven partitioning of the keyspace across machines.
Its performance can also be order-sensitive; for example, pre-aggregation schemes benefit from ``clusteredness'' in their input data, where values with the same key tend to appear close together in the stream.

All of these optimizations involve trade-offs between latency, memory, network usage, and processor time, so the optimal aggregation technique is a function of the data properties and these resources' relative costs.
It's also interesting to consider how adaptive query processing techniques might be applied to aggregation in a streaming setting.

The rest of this report summarizes related work in aggregation optimization, proposes some new optimizations, and explores issues related to aggregation and lineage-based fault-tolerance.

\section{Multi-level Aggregation}

Multi-level aggregation techniques, such aggregation trees, can address
several problems:

\begin{itemize}
    \item In wireless sensor networks, the network may not be fully-connected
    and some pairs of nodes must communicate through other intermediate nodes.
    To save bandwidth (which can lead to large power savings), aggregation can
    be performed at intermediate nodes of the spanning tree \cite{tag}.

    \item Even if all pairs of nodes can directly communicate, the underlying
    network topology may not be flat: for example, intra-rack bandwidth can be
    much greater than intra-rack bandwidth.  In these cases, aggregation trees
    can reduce the amount of data communicated through the cross-rack
    bottleneck.

    \item A node's bandwidth is usually shared by all parties communicating
    with it, so aggregation schemes that send data to a single node may become
    bottlenecked on that machine's individual network link; aggregation trees
    can alleviate this problem by reducing the total amount of data sent to
    any individual machine.
    % TODO: a comment on MPI-style AllReduce might be appropriate somewhere.

    \item For popular keys, aggregation trees can distribute the processing
    cost of applying the aggregate function.

\end{itemize}

Hierarchical aggregation may harm performance if it achieves a low reduction
in network bandwidth. For example, a binary aggregation tree will cause
a logarithmic increase in execution time if the intermediate aggregations
don't reduce the data volume.

\subsection{Aggregation on Multiprocessor Machines}

Multiprocessor machines may produce multiple sets of aggregation outputs and
it is often beneficial to locally combine them.  It may be possible to have
all processors incrementally update the same aggregation data structures, but
this may harm performance through cache invalidation and synchronization
overheads.  Instead, it may be more efficient to give processors their own
aggregation buffers and merge these in a secondary machine-local aggregation
phase.

Sparsity may change these trade-offs: if the key space is large and the
distribution of keys is fairly even, then cache invalidations may already be
common and updates to the shared structures may be less likely to conflict.

\subsection{Network Non-Linearity}

\textbf{TODO:} recap some of John Canney's discussion on networks behaving
non-linearly when sending very small packets, and discuss the implications for
MPI-style AllReduce, etc.

\section{Pre-aggregation}

In distributed engines like MapReduce, these queries can be executed by hash-repartitioning the dataset by its grouping attribute(s), then using a hash-table to compute the aggregates for each group given all of its values.
% TODO: do multiple grouping attributes introduce anything interesting into this scheme?

This can be optimized using \emph{prea-ggregation}, where the aggregation is performed in two phases: first, the subset of a key's values on a particular machine are aggregated into a single value; next, these aggregates are hash-repartitioned and a final aggregation is performed on each group.
This corresponds to Hadoop's ``Combiners'' feature.
For this optimization to apply, the aggregation function has to be commutative and associative.
% TODO: is this strictly true, or can we use a more-precise or weaker set of properties?
% TODO: cite TAG.

Pre-aggregation can improve performance by reducing the volume of data set over the network and by distributing the CPU cost of applying the aggregation function to popular keys.
These benefits come at the cost of additional hash lookups and memory usage during the pre-shuffle phase.

\subsection{Preaggregation and Skew}

Skew can affect preaggregation's cost and benefit.  Key-value pairs can exhibit both input and output skew \cite{adaptive-aggregation}.
With \emph{input skew}, all nodes have the same number of groups but different numbers of records.  Input skew can cause a particular node to become a straggler because it has more input data to process.  In contrast, with \emph{output skew}, nodes have the same number of records but different numbers of groups or distinct keys, which means that some nodes may not have enough buffer space to preaggregate all groups.

\subsection{Partial Preaggregation}

So far, we've considered preaggregation to be an ``all-or-nothing'' optimization that treats all groups similarly.
\emph{Partial preaggregation} \cite{partial-preaggregation} is a middle-ground where multiple values for a group may be emitted during the pre-shuffle phase.  For example, 100 values sharing a particular key might be aggregated into two values instead of one.
This works because the two-phase aggregation strategy still needs to perform a final aggregation, so the ``partial'' preaggregates will be combined during the final aggregation.

Partial preaggregation overcomes limitations on buffer space for holding the pre-shuffle grouping hash table.
While sequentially scanning through records, values can be aggregated into matching entries in the hashtable or inserted as new entries.
When the hashtable becomes full, partial preaggregation algorithms employ an \emph{eviction strategy} to free up space.
The simplest eviction strategies are to evict a randomly-chosen key or to evict all keys.
Values can be evicted by spooling them to disk or by sending them over the network to the node that performs the final aggregation for those keys.

This is exactly analagous to cache eviction policies, where we want to maximize cache hits (preaggregations), and algorithms like LRU or LFU can be used.
The optimal eviction strategy is to evict the key that will be hit furthest in the future; it's not possible to actually implement this strategy, but it provides an upper bound on the effectiveness of our eviction strategy.
% TODO: cite

% TODO: what about lookahead or other secondary buffering techniques?

%Partial preaggregation, \cite{partial-preaggregation} assumes that input records arrive in a random order (i.e. their groups are randomly sampled from some (possibly-skewed) distribution).

\subsection{When to Apply Preaggregation}

To decide whether to apply complete preaggregation or perform a shuffle of the entire dataset, we can calculate the total number of groups being aggregated, since preaggregation is beneficial when there are relatively few keys, but may harm performance on data sets with huge numbers of unique or infrequent keys \cite{adaptive-aggregation}.
Sampling techniques can be used to estimate the number of groups.
Some systems pick an initial aggregation strategy and adaptively change it if the original cost estimates appear to be wrong (e.g. by having nodes autonomously switch from complete pre-aggregation to direct shuffling of tuples if they do not have enough buffer space for the pre-aggregation hashtable) \cite{adaptive-aggregation}.

Partial pre-aggregation is a little trickier because it's sensitive to input orderings.  If the input is perfectly clustered by the grouping attributes, then pre-aggregation requires one partial results' worth of buffer space to achieve the maximum reduction in output size.  Knowing the exact distribution of values across groups doesn't help here, since an adversarial input ordering can lead to high eviction rates.

% TODO: What about multi-phase partial sorting for aggregation?
\section{Additional Optimizations for Pre-aggregation}

\subsection{Allocation of Buffer Space}
There are several tasks to which we can allocate buffer space, including the pre- and post-shuffle hash tables; we can also use buffer space to implement a limited form of ``lookahead'' by dynamically re-ordering arriving tuples before probing the hashtable.  For this analysis, assume that all machines participate in both the pre- and post-shuffle phases.

% Aside: Belady's anomaly shows that increasing buffer space doesn't always improve hit rates / reduce faults under a FIFO replacement policy.  This anomaly doesn't apply to LRU or the optimal replacement policy.  It's worth considering whether the non-optimal heuristics we'll use can suffer from this anomaly.

\subsection{Buffering for Lookahead}

We might consider using a small amount of buffer space to implement ``lookahead'' against the preaggregation hash table.
Does this ever make sense?
Disk drives use a similar idea to serve requests out-of-order to reduce seeks or rotational delays.
Similarly, sorting can reduce cache misses.

With both disks and processor caches, the cost of a ``miss'' is extremely high, yet caches are very expensive so a sort buffer may make sense.
% TODO ... but that's not necessarily the case here because ...
% game out the possibilities as a matrix of outcomes

The trade-off here is one record of additional lookahead versus one hashtable slot, plus the CPU overheads of sorting or otherwise managing the lookahead buffer.

There may be related work in compression literature, since sorting (clustering) reduces the entropy in a dataset and improves its compressibility.

In web caching, people have investigated request reordering approaches where incoming requests can be served out of order \cite{web-caching-new-results}.  In that context, we're concerned with not delaying requests for too long.  This notion of ``too long'' seems to be measured in terms of requests, such that the oldest re-ordered request arrivied at most $r$ requests ago.  The general version of this re-ordering problem is NP-hard (why?).  There are different models for how much space documents take in the cache and the cost of cache misses.  The simplest model, corresponding to our hash-table sizing problem, is the Uniform Model, where all documents have the same size and carry the same cache miss penalty.

The reordering buffer management problem \cite{online-scheduling-for-sorting-buffers} considers requests arriving at a service provider that can benefit from contiguous runs of requests for the same item.  The service provider can buffer a finite number of requests and must evict a buffered request for each incoming request.  The goal is to minimize the number of ``context switches'' that the service provider performs by switching the type of item that it's processing.

A generalization of this proble, the \emph{multi service sorting buffer} problem, has a set of idential service providers.  If requests can't be buffered, this reduces to the classic \emph{paging problem}.  The \emph{queue sorting buffer} problem is another variant where incoming items are appended to queues with fixed space and the service provider must process the head of one of the queues (akin to mergesort).

More generally, our problem appears to be a type of \emph{sorting buffer} or \emph{reordering buffer} problem.

Aha, so the problem that I'm trying to solve is NP-hard since it's the \emph{generalized sorting buffer problem} \cite{sorting-buffer-np-hardness}.
Longest Forward Distance is the optimal offline algorithm for the paging problem \cite{lfd}, but it's not a constant approximation for the sorting buffer problem \cite{sorting-buffer-np-hardness}:

The paper \cite{competitive-reordering-algorithm} considers...

% TODO: have these strategies been explored in disk-spilling hashtables?


\subsection{Selective Bypass}
It may be worth investigating work on \emph{caching with bypassing}.  In many caching scenarios, like memory paging in operating systems, there's no option but to evict from the cache.  Our problem of deciding whether to evict a key or directly forward an input tuple is exactly analgous to the problem of caching with bypassing.

Again, we're faced with a trade-off: if we maintain a list of records with poor hit ratios, the space used to maintain that list could also be used to have a bigger cache.  In processors, you can classify individual instructions in a loop as ``tends to cause cache misses`` (or hits), so you only need a small dense array to track hit rate information \cite{automatic-cache-bypass}.

% TODO: a large portion of the literature here seems to be in patents.

% It looks like a relevant search term is "adaptive cache manamgement strategies"

% TODO: parallelism: can you pipeline this process, having one thread sort pages of records at a time while another thread consumes sorted pages and probes the hashtable?

% TODO: what about associativity?

% \section{Evicting to Local vs. Remote Disk}

% \section{Breaking Ties When Choosing Victims}

% Maybe we could break ties when choosing victims by preferring victims that have been recently evicted (a sort of ``poor get poorer'' approach).

\pagebreak

\section{Recovery}

\textbf{TODO:} Discuss effects of ordering on recoverability.  Ties into ideas
from materialized view maintenance.  Discuss the ``taint tracking'' idea in
the context of Spark lineage for accumulator updates.

\section{Optimizations for Skewed Data}

\cite{perf-eval-heavy-tail} describes several applications in networking,
load-balancing, and scheduling where awareness of skewed data distribution
enables optimizations.

These optimizations usually work by identifying the most popular keys or data
elements and processing them differently.

\textbf{TODO:} talk about how this trick can be used to adaptively apply many
other types of aggregation optimizations (e.g. shared and synchronized (or
CAS) data structures for infrequent, long-tailed keys, and separate,
cache-locality-preserving structures for the popular keys).

%\subsection{Machine Learning}

%Skew in machine learning: skew can cause problems for classification tasks.
%For example, in fraud detection systems, most transactions are not
%fraudulent, so a naive algorithm that always says ``not fraudulent'' will be
%correct most of the time
%(http://dl.acm.org/citation.cfm?id=1007730.1007738).  In practice,
%techniques can be used to address the imbalance in the underlying data, such
%as performing biased sampling so the fraudulent training examples will be
%weighted more heavily.

\subsection{Join Queries}

There are several join optimizations for skewed data.  For example,
\cite{smj-skew-optmization} modifies sort-merge join to devote more resources
to processing popular keys.
% TODO: I need to read through the SMJ optimization papers in more detail.

Apache Hive \cite{hive-join-strategies} employs a scheme that uses
a shuffle-based join for most keys, but defers joins for the extremely
frequent keys.  These keys are joined in a separate map-join phase.  This
approach prevents a single popular key from bottlenecking at a single reducer.

% Skew Handling Techniques in Sort-Merge Joins: http://www.cs.arizona.edu/people/dgao/pub/skew.pdf

% In the AQP survey framework, this is an adaptive technique that's based on \emph{horizontal partitioning}, picking different traditional query plans for disjoint portions of the query's data.

\pagebreak
\section{Analysis}

\subsection{Payoffs in terms of data properties}

\subsection{Characterization of optimal and default strategies}

\subsection{Permutations of local data}

Consider a data set of $n$ key-value pairs, with $k$ distinct keys.

Assume that keys are drawn from a Zipf distribution with parameter $\alpha$.
Using complete pre-aggregation on data drawn from this distribution, what's the best-case reduction of tuples in the input to the final aggregation phase?
For large values of $\alpha$, the keys are sampled uniformly, so each key appears roughly $n/k$ times and complete pre-aggregation reduces the final reducer's input by a factor of $n/k$.
At the other extreme, a small $\alpha$ produces a handful of popular keys and a long tail of unique or infrequent keys.

Actually, complete pre-aggregation always reduces the size by a factor of $n/k$, since we output exactly one tuple for each of the $k$ keys.

%To be precise, if key $i$ appears with frequency $d_i$, then complete pre-aggregation outputs exactly $k$ tuples, one per key, saving
%\[
%    \sum_{i=1}^k \min \left(d_i - 1, 0\right) = 
%\]

If we draw keys from this Zipf distribution, what's the expected number of unique keys that are assigned to each of $m$ machines?
\begin{align*}
    E[X|\alpha] &= \sum_{k=1}^K E[I(k, d_k) | \alpha]
\\              &= \sum_{k=1}^K \sum_{d=1}^K \left(1 - \left(1 - 1/m\right)^d\right)p(d | \alpha)
\\              &= k \sum_{k=1}^K\left(1 - p\left(\text{all copies of $k$ are assigned to other machines}\right)\right) p(d | \alpha)
\end{align*}

What's the maximum number of unique keys that could be assigned to one machine?  In principle, all keys could appear on one machine, so we'll look at the distribution of the maximum number of keys per machine.


\pagebreak
\section{Tricks}

\subsection{Affinity Routing}

\subsection{Multi-level aggregation with eviction and skip levels}

\subsection{Sketching for frequent values}


\section{Adaptivity}

The optimal aggregation strategy depends on the number of groups and their
distribution across nodes.
For example, it only makes sense to use Combiners in MapReduce when the number of groups is small.
Using global statistics, we could try to estimate the number of groups to decide whether to use combiners.
Alternatively, we can use adaptive aggregation algorithms that allow individual nodes to pick their aggregation strategies based on the properties of their local input data \cite{adaptive-aggregation}.

% TODO: borrowing from other AQP work, maybe we could use only a sample of the input stream to maintain hit rate statistics.

\section{Order Sensitivity}
Preaggregation techniques are sensitive to the arrival order of tuples.  With partial preaggregation, for example, the output size may be minimal if the tuples arrive in sorted order, but can be much larger with other orderings \cite{partial-preaggregation}.

When analyzing the expected output reduction due to partial preaggregation, \cite{partial-preaggregation} assumes that input records arrive in a random order (i.e. their groups are randomly sampled from some (possibly-skewed) distribution).

Fully-sorted input will achieve optimal aggregation performance, but partially-sorted orders are still interesting to analyze.  Often, real-world data exhibits temporal clustering; for example, interaction data from a single user's shopping session is likely to be close together in a clickstream log.  To analyze these cases, \cite{estimating-cardinality} introduces a \emph{clusteredness} measurement and explores its impact on the performance of partial preaggregation algorithms.  The clusteredness metric considers only the clustering of input groups and not their relative sort orderings.

\subsection{Clusteredness}

Find the positions where the group changes.  Add these up and exclude the first appearance of each group.  Then, perfectly clustered data has zero changes.  By itself, this is an insufficient measure of clusteredness because it doesn't consider \emph{which} groups we're changing between (e.g. cycling between a few groups vs. changing uniformly at random).  So, we extend this to incorporate the average number of changes until members of the same group are seen.  In short, the clusteredness ``expresses the average number of changes between the appearance of two identical grouping values.''  A clusteredeness of $d$ means that, on average, $d$ group changes will occur until a group is re-encountered.


\textbf{Distance classes:} distances between appearances may have high variance, so we replace a single average $d$ with a set of average distances over different ranges (this is like creating a histogram using more bins).

\textbf{Group classes:} ???

The paper doesn't explore how to efficiently compute the clusteredness, so I'm doubtful of this technique's usefulness in practice.

% TODO
\textbf{Question}: what's the actual clusteredness of real data sets?  Can we compute this and just report some empirical results, since these would establish more realistic bounds for real problem instances?

\subsection{Bounds}

\cite{estimating-cardinality}
Randomized replacement represents an upper bound on the output size; eviction using lookahead gives a lower bound, even though it's impossible to implement in practice.


\bibliographystyle{plain}
\bibliography{aggregation}

\end{document}
